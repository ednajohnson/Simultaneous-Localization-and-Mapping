{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Late fusion.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","outputId":"83f9148d-7f15-4792-aa06-11eab780e2ce","executionInfo":{"status":"ok","timestamp":1579665613519,"user_tz":240,"elapsed":6691,"user":{"displayName":"edna johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3lT2kvmIBG8PROxu_-19mMNzYd_8kTUqFYLDQ=s64","userId":"03197522840940088727"}},"id":"33Y01jju_xEG","colab":{"base_uri":"https://localhost:8080/","height":140}},"source":["# Load all dependency\n","%load_ext autoreload\n","%autoreload 2\n","\n","!pip install torchviz\n","!pip install transforms3d\n","\n","from IPython.display import HTML\n","from IPython.display import clear_output\n","\n","import numpy as np\n","\n","import torch\n","from torch import nn\n","from torch.nn import modules\n","from torchvision import transforms, models\n","import torchvision.utils as vutils\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","from torchviz import make_dot\n","\n","from datasets.apolloscape import Apolloscape\n","\n","from utils.common import draw_poses\n","from utils.common import draw_record\n","from utils.common import imshow\n","from utils.common import save_checkpoint\n","from utils.common import AverageMeter\n","from utils.common import calc_poses_params, quaternion_angular_error\n","\n","from models.posenet import PoseNet, PoseNetCriterion\n","from models.posenet1 import PoseNet1\n","from models.posenet2 import PoseNet2\n","\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","from PIL import Image\n","\n","from tqdm import tqdm\n","import os\n","import time\n","from datetime import datetime\n","\n","%matplotlib inline\n","plt.ion()\n","\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import LabelEncoder\n","\n","from mlxtend.classifier import StackingClassifier\n","from mlxtend.feature_selection import ColumnSelector\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n","Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.17.5)\n","Requirement already satisfied: transforms3d in /usr/local/lib/python3.6/dist-packages (0.3.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hZJIYV6kzoNa","colab_type":"code","outputId":"717162c9-f50c-488b-e32e-ac7d764dd51e","executionInfo":{"status":"ok","timestamp":1579665634131,"user_tz":240,"elapsed":27278,"user":{"displayName":"edna johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3lT2kvmIBG8PROxu_-19mMNzYd_8kTUqFYLDQ=s64","userId":"03197522840940088727"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1wxfUFDnzpCB","colab_type":"code","outputId":"93c149fc-197c-4bc9-a95a-d71437e5f7ab","executionInfo":{"status":"ok","timestamp":1579665671059,"user_tz":240,"elapsed":64183,"user":{"displayName":"edna johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3lT2kvmIBG8PROxu_-19mMNzYd_8kTUqFYLDQ=s64","userId":"03197522840940088727"}},"colab":{"base_uri":"https://localhost:8080/","height":389}},"source":["APOLLO_PATH = \"./drive/My Drive/Colab Notebooks/Final_Project/apolloscape\"\n","\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225])\n","\n","# Resize data before using\n","transform = transforms.Compose([\n","    transforms.Resize(260),\n","    transforms.CenterCrop(250),\n","    transforms.ToTensor(),\n","    normalize\n","])\n","\n","experiment_name = 'zpark_L6'\n","\n","pretrained = True\n","stereo = False\n","shuffle = True\n","\n","batch_size = 40\n","\n","train_record = None # 'Record001'\n","train_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark-sample\",\n","                             transform=transform, record=train_record, normalize_poses=True,\n","                             pose_format='quat', train=True, cache_transform=False, stereo=stereo)\n","\n","val_record = None # 'Record013'\n","val_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=\"zpark-sample\",\n","                             transform=transform, record=val_record, normalize_poses=True,\n","                             pose_format='quat', train=False, cache_transform=False, stereo=stereo)\n","\n","\n","print(train_dataset)\n","print(val_dataset)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle) # batch_size = 75\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle) # batch_size = 75"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Dataset: Apolloscape\n","    Road: zpark-sample\n","    Record: None\n","    Train: True\n","    Normalize Poses: True\n","    Stereo: False\n","    Length: 2242 of 2242\n","    Cameras: ['Camera_2', 'Camera_1']\n","    Records: ['Record001', 'Record002', 'Record003', 'Record004', 'Record006', 'Record007', 'Record008', 'Record009', 'Record010', 'Record011', 'Record012', 'Record013', 'Record014']\n","\n","Dataset: Apolloscape\n","    Road: zpark-sample\n","    Record: None\n","    Train: False\n","    Normalize Poses: True\n","    Stereo: False\n","    Length: 756 of 756\n","    Cameras: ['Camera_2', 'Camera_1']\n","    Records: ['Record001', 'Record002', 'Record003', 'Record004', 'Record006', 'Record007', 'Record008', 'Record009', 'Record010', 'Record011', 'Record012', 'Record013', 'Record014']\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SSYWva6wHD-a","colab_type":"code","outputId":"1754882e-6bed-4615-9a1b-05ff4c3a5cfa","executionInfo":{"status":"ok","timestamp":1579665671061,"user_tz":240,"elapsed":64163,"user":{"displayName":"edna johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3lT2kvmIBG8PROxu_-19mMNzYd_8kTUqFYLDQ=s64","userId":"03197522840940088727"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device = None\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print('device = {}'.format(device))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["device = cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B4tivGDn2DPw","colab_type":"code","outputId":"9cb2ceea-8aac-446d-dbff-f8848b63b4ec","executionInfo":{"status":"ok","timestamp":1579665727284,"user_tz":240,"elapsed":120361,"user":{"displayName":"edna johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3lT2kvmIBG8PROxu_-19mMNzYd_8kTUqFYLDQ=s64","userId":"03197522840940088727"}},"colab":{"base_uri":"https://localhost:8080/","height":193}},"source":["# Create pretrained feature extractor\n","# feature_extractor = models.resnet18(pretrained=True)\n","from torch.autograd import Variable\n","\n","feature_extractor_1 = models.resnet18(pretrained=True)\n","feature_extractor_2 = models.resnet34(pretrained=True)\n","feature_extractor_3 = models.resnet101(pretrained=True)\n","feature_extractor_4 = models.vgg16(pretrained=True)\n","#feature_extractor_5 = models.alexnet(pretrained=True)\n","feature_extractor_6 = models.vgg19(pretrained=True)\n","#feature_extractor_7 = models.resnet50(pretrained=True)\n","num_features = 2048\n","\n","# Create model\n","model1 = PoseNet(feature_extractor_1, num_features=num_features, pretrained=pretrained)\n","model1 = model1.to(device)\n","\n","model2 = PoseNet(feature_extractor_2, num_features=num_features, pretrained=pretrained)\n","model2 = model2.to(device)\n","\n","model3 = PoseNet(feature_extractor_3, num_features=num_features, pretrained=pretrained)\n","model3 = model3.to(device)\n","\n","model4 = PoseNet1(feature_extractor_4, num_features=num_features, pretrained=pretrained)\n","model4 = model4.to(device)\n","\n","#model5 = PoseNet2(feature_extractor_5, num_features=num_features, pretrained=pretrained)\n","#model5 = model5.to(device)\n","\n","model6 = PoseNet1(feature_extractor_6, num_features=num_features, pretrained=pretrained)\n","model6 = model6.to(device)\n","\n","#model7 = PoseNet(feature_extractor_7, num_features=num_features, pretrained=pretrained)\n","#model7 = model7.to(device)\n","\n","start_epoch = 0\n","\n","# Criterion\n","criterion_1 = PoseNetCriterion(stereo=stereo, learn_beta=True)\n","criterion_1 = criterion_1.to(device)\n","\n","criterion_2 = PoseNetCriterion(stereo=stereo, learn_beta=True)\n","criterion_2 = criterion_2.to(device)\n","\n","criterion_3 = PoseNetCriterion(stereo=stereo, learn_beta=True)\n","criterion_3 = criterion_3.to(device)\n","\n","criterion_4 = PoseNetCriterion(stereo=stereo, learn_beta=True)\n","criterion_4 = criterion_4.to(device)\n","\n","#criterion_5 = PoseNetCriterion(stereo=stereo, learn_beta=True)\n","#criterion_5 = criterion_5.to(device)\n","\n","criterion_6 = PoseNetCriterion(stereo=stereo, learn_beta=True)\n","criterion_6 = criterion_6.to(device)\n","\n","#criterion_7 = PoseNetCriterion(stereo=stereo, learn_beta=True)\n","#criterion_7 = criterion_7.to(device)\n","# Add all params for optimization\n","\n","param_list_1 = [{'params': model1.parameters()}]\n","if criterion_1.learn_beta:\n","    param_list_1.append({'params': criterion_1.parameters()})\n","\n","param_list_2 = [{'params': model2.parameters()}]\n","if criterion_2.learn_beta:\n","    param_list_2.append({'params': criterion_2.parameters()})\n","\n","param_list_3 = [{'params': model3.parameters()}]\n","if criterion_1.learn_beta:\n","    param_list_3.append({'params': criterion_3.parameters()})\n","\n","param_list_4 = [{'params': model4.parameters()}]\n","if criterion_4.learn_beta:\n","    param_list_4.append({'params': criterion_4.parameters()})\n","\n","#param_list_5 = [{'params': model5.parameters()}]\n","#if criterion_5.learn_beta:\n","#    param_list_5.append({'params': criterion_5.parameters()})\n","\n","param_list_6 = [{'params': model6.parameters()}]\n","if criterion_6.learn_beta:\n","    param_list_6.append({'params': criterion_6.parameters()})\n","\n","#param_list_7 = [{'params': model7.parameters()}]\n","#if criterion_7.learn_beta:\n","#    param_list_7.append({'params': criterion_7.parameters()})\n","\n","# Create optimizer\n","optimizer_1 = optim.Adam(params=param_list_1, lr=1e-5, weight_decay=0.0005)\n","optimizer_2 = optim.Adam(params=param_list_2, lr=1e-5, weight_decay=0.0005)\n","optimizer_3 = optim.Adam(params=param_list_3, lr=1e-5, weight_decay=0.0005)\n","optimizer_4 = optim.Adam(params=param_list_4, lr=1e-5, weight_decay=0.0005)\n","#optimizer_5 = optim.Adam(params=param_list_5, lr=1e-5, weight_decay=0.0005)\n","optimizer_6 = optim.Adam(params=param_list_6, lr=1e-5, weight_decay=0.0005)\n","#optimizer_7 = optim.Adam(params=param_list_7, lr=1e-5, weight_decay=0.0005)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n","100%|██████████| 44.7M/44.7M [00:01<00:00, 41.1MB/s]\n","Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n","100%|██████████| 83.3M/83.3M [00:00<00:00, 154MB/s]\n","Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n","100%|██████████| 170M/170M [00:03<00:00, 55.6MB/s]\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:08<00:00, 63.3MB/s]\n","Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n","100%|██████████| 548M/548M [00:26<00:00, 21.9MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"cljSBDv_BE9y","colab_type":"code","outputId":"4343fd20-62bb-4c27-88eb-ab239b598d7c","executionInfo":{"status":"ok","timestamp":1579665741890,"user_tz":240,"elapsed":134935,"user":{"displayName":"edna johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3lT2kvmIBG8PROxu_-19mMNzYd_8kTUqFYLDQ=s64","userId":"03197522840940088727"}},"colab":{"base_uri":"https://localhost:8080/","height":193}},"source":["# Restore from checkpoint\n","# checkpoint_file = '_checkpoints/20180823_085404_zpark_posenet_L1_resnet34p_2048_e2880.pth.tar'\n","checkpoint_file_1 = './drive/My Drive/Colab Notebooks/Resnet18Model.pth.tar'\n","\n","if 'checkpoint_file_1' in locals() and checkpoint_file_1 is not None:\n","    if os.path.isfile(checkpoint_file_1):\n","        print('Loading from checkpoint: {}'.format(checkpoint_file_1))\n","        checkpoint_1 = torch.load(checkpoint_file_1, map_location=torch.device('cpu'))\n","        model1.load_state_dict(checkpoint_1['model_state_dict'])\n","        optimizer_1.load_state_dict(checkpoint_1['optim_state_dict'])\n","        start_epoch = checkpoint_1['epoch']\n","        if 'criterion_state_dict' in checkpoint_1:\n","            criterion_1.load_state_dict(checkpoint_1['criterion_state_dict'])\n","            print('Loaded params.')\n","\n","checkpoint_file_2 = './drive/My Drive/Colab Notebooks/Resnet34Model.pth.tar'\n","\n","if 'checkpoint_file_2' in locals() and checkpoint_file_2 is not None:\n","    if os.path.isfile(checkpoint_file_2):\n","        print('Loading from checkpoint: {}'.format(checkpoint_file_2))\n","        checkpoint_2 = torch.load(checkpoint_file_2, map_location=torch.device('cpu'))\n","        model2.load_state_dict(checkpoint_2['model_state_dict'])\n","        optimizer_2.load_state_dict(checkpoint_2['optim_state_dict'])\n","        start_epoch = checkpoint_2['epoch']\n","        if 'criterion_state_dict' in checkpoint_2:\n","            criterion_2.load_state_dict(checkpoint_2['criterion_state_dict'])\n","            print('Loaded params.')\n","\n","checkpoint_file_3 = './drive/My Drive/Colab Notebooks/Resnet101Model.pth.tar'\n","\n","if 'checkpoint_file_3' in locals() and checkpoint_file_3 is not None:\n","    if os.path.isfile(checkpoint_file_3):\n","        print('Loading from checkpoint: {}'.format(checkpoint_file_3))\n","        checkpoint_3 = torch.load(checkpoint_file_3, map_location=torch.device('cpu'))\n","        model3.load_state_dict(checkpoint_3['model_state_dict'])\n","        optimizer_3.load_state_dict(checkpoint_3['optim_state_dict'])\n","        start_epoch = checkpoint_3['epoch']\n","        if 'criterion_state_dict' in checkpoint_3:\n","            criterion_3.load_state_dict(checkpoint_3['criterion_state_dict'])\n","            print('Loaded params.')\n","\n","checkpoint_file_4 = './drive/My Drive/Colab Notebooks/Vgg16Model.pth.tar'\n","\n","if 'checkpoint_file_4' in locals() and checkpoint_file_4 is not None:\n","    if os.path.isfile(checkpoint_file_4):\n","        print('Loading from checkpoint: {}'.format(checkpoint_file_4))\n","        checkpoint_4 = torch.load(checkpoint_file_4, map_location=torch.device('cpu'))\n","        model4.load_state_dict(checkpoint_4['model_state_dict'])\n","        optimizer_4.load_state_dict(checkpoint_4['optim_state_dict'])\n","        start_epoch = checkpoint_4['epoch']\n","        if 'criterion_state_dict' in checkpoint_4:\n","            criterion_4.load_state_dict(checkpoint_4['criterion_state_dict'])\n","            print('Loaded params.')\n","\n","#checkpoint_file_7 = './drive/My Drive/Colab Notebooks/Resnet50Model.pth.tar'\n","\n","#if 'checkpoint_file_7' in locals() and checkpoint_file_7 is not None:\n","#    if os.path.isfile(checkpoint_file_7):\n","#        print('Loading from checkpoint: {}'.format(checkpoint_file_7))\n","#        checkpoint_7 = torch.load(checkpoint_file_7, map_location=torch.device('cpu'))\n","#        model7.load_state_dict(checkpoint_7['model_state_dict'])\n","#        optimizer_7.load_state_dict(checkpoint_7['optim_state_dict'])\n","#        start_epoch = checkpoint_7['epoch']\n","#        if 'criterion_state_dict' in checkpoint_7:\n","#            criterion_7.load_state_dict(checkpoint_7['criterion_state_dict'])\n","#            print('Loaded params.')\n","\n","checkpoint_file_6 = './drive/My Drive/Colab Notebooks/Vgg19Model.pth.tar'\n","\n","if 'checkpoint_file_6' in locals() and checkpoint_file_6 is not None:\n","    if os.path.isfile(checkpoint_file_6):\n","        print('Loading from checkpoint: {}'.format(checkpoint_file_6))\n","        checkpoint_6 = torch.load(checkpoint_file_6, map_location=torch.device('cpu'))\n","        model6.load_state_dict(checkpoint_6['model_state_dict'])\n","        optimizer_6.load_state_dict(checkpoint_6['optim_state_dict'])\n","        start_epoch = checkpoint_6['epoch']\n","        if 'criterion_state_dict' in checkpoint_6:\n","            criterion_6.load_state_dict(checkpoint_6['criterion_state_dict'])\n","            print('Loaded params.')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Loading from checkpoint: ./drive/My Drive/Colab Notebooks/Resnet18Model.pth.tar\n","Loaded params.\n","Loading from checkpoint: ./drive/My Drive/Colab Notebooks/Resnet34Model.pth.tar\n","Loaded params.\n","Loading from checkpoint: ./drive/My Drive/Colab Notebooks/Resnet101Model.pth.tar\n","Loaded params.\n","Loading from checkpoint: ./drive/My Drive/Colab Notebooks/Vgg16Model.pth.tar\n","Loaded params.\n","Loading from checkpoint: ./drive/My Drive/Colab Notebooks/Vgg19Model.pth.tar\n","Loaded params.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"efyhSRwu2_IX","colab_type":"code","colab":{}},"source":["def model_results_pred_gt(model, dataloader, poses_mean, poses_std, stereo=True):\n","    model.eval()\n","\n","    gt_poses = np.empty((0, 7))\n","    pred_poses = np.empty((0, 7))\n","\n","    for idx, (batch_images, batch_poses) in enumerate(dataloader):\n","        \n","        if stereo:\n","            batch_images = [x.to(device) for x in batch_images]\n","            batch_poses = [x.to(device) for x in batch_poses]\n","        else:\n","            batch_images = batch_images.to(device)\n","            batch_poses = batch_poses.to(device)\n","\n","\n","        out = model(batch_images)\n","        \n","        loss = criterion_1(out, batch_poses)\n","\n","        # move data to cpu & numpy\n","        if stereo:\n","            batch_poses = [x.detach().cpu().numpy() for x in batch_poses]\n","            out = [x.detach().cpu().numpy() for x in out]\n","            gt_poses = np.vstack((gt_poses, *batch_poses))\n","            pred_poses = np.vstack((pred_poses, *out))\n","        else:\n","            bp = batch_poses.detach().cpu().numpy()\n","            outp = out.detach().cpu().numpy()\n","            gt_poses = np.vstack((gt_poses, bp))\n","            pred_poses = np.vstack((pred_poses, outp))\n","\n","\n","        \n","    # un-normalize translation\n","    gt_poses[:, :3] = gt_poses[:, :3] * poses_std + poses_mean\n","    pred_poses[:, :3] = pred_poses[:, :3] * poses_std + poses_mean\n","    \n","    return pred_poses, gt_poses"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BiaYoQA9oLGS","colab_type":"code","outputId":"5f3a1b04-47c3-4c66-b35b-7edbe16a538a","executionInfo":{"status":"ok","timestamp":1579666421938,"user_tz":240,"elapsed":666224,"user":{"displayName":"edna johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3lT2kvmIBG8PROxu_-19mMNzYd_8kTUqFYLDQ=s64","userId":"03197522840940088727"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["#Get mean and std from dataset\n","poses_mean = val_dataset.poses_mean\n","poses_std = val_dataset.poses_std\n","\n","print('\\n=== Test Validation Dataset ======')\n","pred_poses_1, gt_poses_1 = model_results_pred_gt(model1, val_dataloader, poses_mean, poses_std, stereo=stereo)\n","pred_poses_2, gt_poses_2 = model_results_pred_gt(model2, val_dataloader, poses_mean, poses_std, stereo=stereo)\n","pred_poses_3, gt_poses_3 = model_results_pred_gt(model3, val_dataloader, poses_mean, poses_std, stereo=stereo)\n","pred_poses_4, gt_poses_4 = model_results_pred_gt(model4, val_dataloader, poses_mean, poses_std, stereo=stereo)\n","#pred_poses_5, gt_poses_5 = model_results_pred_gt(model5, val_dataloader, poses_mean, poses_std, stereo=stereo)\n","pred_poses_6, gt_poses_6 = model_results_pred_gt(model6, val_dataloader, poses_mean, poses_std, stereo=stereo)\n","#pred_poses_7, gt_poses_7 = model_results_pred_gt(model7, val_dataloader, poses_mean, poses_std, stereo=stereo)\n","\n","pred_poses = (pred_poses_1 + pred_poses_2 + pred_poses_3 + pred_poses_4 + pred_poses_6)/5\n","gt_poses = (gt_poses_1 + gt_poses_2 + gt_poses_3 + gt_poses_4 + gt_poses_6)/5\n","\n","#np.savetxt(\"pred_poses.csv\", pred_poses, delimiter=',', comments=\"\")\n","#np.savetxt(\"gt_poses.csv\", gt_poses, delimiter=',', comments=\"\")\n","\n","print('gt_poses = {}'.format(gt_poses.shape))\n","print('pred_poses = {}'.format(pred_poses.shape))\n","t_loss = np.asarray([np.linalg.norm(p - t) for p, t in zip(pred_poses[:, :3], gt_poses[:, :3])])\n","q_loss = np.asarray([quaternion_angular_error(p, t) for p, t in zip(pred_poses[:, 3:], gt_poses[:, 3:])])\n","\n","print('poses_std = {:.3f}'.format(np.linalg.norm(poses_std)))\n","print('Translation(T) error in meters and Rotation(R) error in degrees:')\n","print('T: median = {:.3f}, mean = {:.3f}'.format(np.median(t_loss), np.mean(t_loss)))\n","print('R: median = {:.3f}, mean = {:.3f}'.format(np.median(q_loss), np.mean(q_loss)))\n","\n","# Save for later visualization\n","#pred_poses_val = pred_poses\n","#gt_poses_val = gt_poses"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\n","=== Test Validation Dataset ======\n","gt_poses = (756, 7)\n","pred_poses = (756, 7)\n","poses_std = 280.970\n","Translation(T) error in meters and Rotation(R) error in degrees:\n","T: median = 9.288, mean = 10.352\n","R: median = 0.931, mean = 4.151\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9V198ML6dWMR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":246},"outputId":"b2d857ec-e012-4ddf-86dd-6610138b8b3d","executionInfo":{"status":"ok","timestamp":1579667053282,"user_tz":240,"elapsed":614,"user":{"displayName":"edna johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3lT2kvmIBG8PROxu_-19mMNzYd_8kTUqFYLDQ=s64","userId":"03197522840940088727"}}},"source":["data1 = np.loadtxt('pred_poses_1')\n","print(data1)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["[[ 5.47409564e+02 -2.22899669e+03  4.02282543e+01 ... -2.87257016e-01\n","   7.22187877e-01 -5.86950064e-01]\n"," [ 3.26294691e+02 -2.39396893e+03  4.06166079e+01 ... -7.79828072e-01\n","  -1.62897632e-03  1.92154981e-02]\n"," [ 5.34936891e+02 -2.10705136e+03  4.01266676e+01 ... -1.33494392e-01\n","  -7.63605416e-01  6.13447189e-01]\n"," ...\n"," [ 3.52307500e+02 -1.91605973e+03  4.00668908e+01 ... -5.63804507e-01\n","   5.41498005e-01 -4.25178409e-01]\n"," [ 3.66331748e+02 -2.61325011e+03  4.02018798e+01 ... -7.28249133e-01\n","  -2.59955198e-01  1.61541909e-01]\n"," [ 3.13951524e+02 -2.30156566e+03  4.05283709e+01 ... -7.85009861e-01\n","  -4.05925512e-03  2.00082511e-02]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jdEr8Uesfk0N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":246},"outputId":"5b8c214e-672b-49ab-8b3d-2aa1a0c5dc4e","executionInfo":{"status":"ok","timestamp":1579667057714,"user_tz":240,"elapsed":582,"user":{"displayName":"edna johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3lT2kvmIBG8PROxu_-19mMNzYd_8kTUqFYLDQ=s64","userId":"03197522840940088727"}}},"source":["print(pred_poses_1)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[[ 5.47409564e+02 -2.22899669e+03  4.02282543e+01 ... -2.87257016e-01\n","   7.22187877e-01 -5.86950064e-01]\n"," [ 3.26294691e+02 -2.39396893e+03  4.06166079e+01 ... -7.79828072e-01\n","  -1.62897632e-03  1.92154981e-02]\n"," [ 5.34936891e+02 -2.10705136e+03  4.01266676e+01 ... -1.33494392e-01\n","  -7.63605416e-01  6.13447189e-01]\n"," ...\n"," [ 3.52307500e+02 -1.91605973e+03  4.00668908e+01 ... -5.63804507e-01\n","   5.41498005e-01 -4.25178409e-01]\n"," [ 3.66331748e+02 -2.61325011e+03  4.02018798e+01 ... -7.28249133e-01\n","  -2.59955198e-01  1.61541909e-01]\n"," [ 3.13951524e+02 -2.30156566e+03  4.05283709e+01 ... -7.85009861e-01\n","  -4.05925512e-03  2.00082511e-02]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"it5jAt3mdlWB","colab_type":"code","colab":{}},"source":["np.savetxt(\"pred_poses_1\", pred_poses_1)\n","np.savetxt(\"pred_poses_2\", pred_poses_2)\n","np.savetxt(\"pred_poses_3\", pred_poses_3)\n","np.savetxt(\"pred_poses_4\", pred_poses_4)\n","np.savetxt(\"pred_poses_6\", pred_poses_6)\n","\n","np.savetxt(\"gt_poses_1\", gt_poses_1)\n","np.savetxt(\"gt_poses_2\", gt_poses_2)\n","np.savetxt(\"gt_poses_3\", gt_poses_3)\n","np.savetxt(\"gt_poses_4\", gt_poses_4)\n","np.savetxt(\"gt_poses_6\", gt_poses_6)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hEJuQcKofW-U","colab_type":"code","colab":{}},"source":["np.savetxt(\"pred_poses_1\", pred_poses_1)"],"execution_count":0,"outputs":[]}]}